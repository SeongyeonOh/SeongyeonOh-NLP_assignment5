{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction \n",
        "\n",
        "DialoGPT(Dialogue Generative Pre-trained Transformer)는 neural conversational response generation을 다루기 위해 GPT-2를 확장한다.\n",
        "\n",
        "GPT-2와 같이 DialoGPT는 Auto Regressive(AR) Language Model(LM)이며 multi-layer transformer를 사용한다. 그러나 GPT-2와 다르게 Reddit discussion chain에서 추출된 대규모 대화 pairs/sessions으로 학습된다. 논문에서는 이 대규모 대화 pairs/sessions이 DialoGPT가 대화 흐름에서 $P(Target, Source)$에 대한 joint distribution을 포착할 수 있게 만들었다. "
      ],
      "metadata": {
        "id": "3xnJF_8Gcf40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "\n",
        "데이터셋은 2005년부터 2017년에 걸쳐 Reddit에서 스크랩된 comment chain에서 추출된다. 아래 기준에 해당하는 데이터를 필터링한다.\n",
        "\n",
        "\n",
        "\n",
        "1.   URL이 있는 source나 target\n",
        "2.   3개 이상의 단어 반복이 target에 포함된 경우\n",
        "3.   응답이 가장 자주 사용하는 top 50 단어(a, the, of 등)가 적어도 하나 이상 포함되지 않은 경우 \n",
        "4.   응답에 [\"또는\"] 이 포함된 경우\n",
        "5.   source와 target 시퀀스가 합쳐서 200단어보다 긴 경우\n",
        "6.   target이 offensive language를 포함한 경우\n",
        "7.   하위 레딧에 많은 수가 offensive한 내용을 포함할 가능성이 많다고 인식되는 경우\n",
        "8.   단조로운 문장 적극적으로 배제 \n",
        "\n",
        "\n",
        "필터링 후 데이터 세트는 총 18억 개의 단어로 147,116,725개의 대화 인스턴스로 구성된다. \n",
        "\n"
      ],
      "metadata": {
        "id": "La0JW4hBcftx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method\n",
        "\n",
        "- Model architecture\n",
        "\n",
        "  - 우리는 첫 번째로 대화세션 안에서 모든 대화 turns을 concat 시켜 긴 text $x_1, ..., x_N$($N$은 시퀀스 길이)를 만들고 끝에는 end-of-text-token을 넣는다. \n",
        "  - source sentence(대화 히스토리)는 $S = x_1, ..., x_m$으로 표기하고 target sentence (ground-truth response)는 $T = x_{m+1}, ..., x_N$으로 표기한다.\n",
        "  - 이때, $P(T|S)$은 조건부 확률의 일련의 곱으로 아래의 식과 같이 쓰여진다. \n",
        "  > $p(T|s) = \\prod_{n = m+1}^N p(x_n | x_1,..., x_{n-1}) $\n",
        "  - DialoGPT는 GPT-2를 따라 multi-turn dialogue를 하나의 text로 간주한다.\n",
        "  - 따라서 multi-turn dialogue session인 $T_1, ..., T_k$은 $p(T_k, ..., T_2|T_1)$로 볼 수 있고 이는 사실 $p(T_i|T_1,...,T_{i-1})$ (여기서 $i$는 $m+1$) 조건부 확률을 product한 것이다. \n",
        "  - 결과적으로 $p(T_k, ..., T_2|T_1)$을 최적화하는 것은 모든 $p(T_I | T_1, ..., T_{i-1})$ source-target 페어를 최적화하는 것이다.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zOox7kQhhHpu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code"
      ],
      "metadata": {
        "id": "k7iGXvhFGAJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount"
      ],
      "metadata": {
        "id": "2Xqd1HvtGDSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "FE9xdRecEANb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install & Import library"
      ],
      "metadata": {
        "id": "WJO-W0BxGFDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "3_qGCUsoDWDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/drive/MyDrive/Dialo-20221218T162952Z-001.zip'"
      ],
      "metadata": {
        "id": "o22EoYNME7yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/Dialo'"
      ],
      "metadata": {
        "id": "L_6u_FGsEE6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv config.py medium/\n",
        "!mv discord_bot.py medium/\n",
        "!mv interact.py medium/"
      ],
      "metadata": {
        "id": "EQC_IDCMMguF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/Dialo/medium'"
      ],
      "metadata": {
        "id": "egmb0_99LcDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run \"interact.py\"\n",
        "\n",
        "py파일을 바로 돌려도 된다\n",
        "\n"
      ],
      "metadata": {
        "id": "Kndyq1mtPjNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\n",
        "from config import device_f, device_r, num_samples, MMI_temperature, top_k\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "tokenizer = GPT2Tokenizer('medium/vocab.json', 'medium/merges.txt')\n",
        "\n",
        "weights = torch.load('medium/medium_ft.pkl')\n",
        "# fix misused key value\n",
        "weights[\"lm_head.weight\"] = weights[\"lm_head.decoder.weight\"]\n",
        "weights.pop(\"lm_head.decoder.weight\", None)\n",
        "\n",
        "cfg = GPT2Config.from_json_file('medium/config.json')\n",
        "model: GPT2LMHeadModel = GPT2LMHeadModel(cfg)\n",
        "model.load_state_dict(weights,strict=False)\n",
        "if device_f == 'cuda':\n",
        "    model.half()\n",
        "model.to(device_f)\n",
        "model.eval()\n",
        "\n",
        "weights = torch.load('medium/small_reverse.pkl')\n",
        "# fix misused key value\n",
        "weights[\"lm_head.weight\"] = weights[\"lm_head.decoder.weight\"]\n",
        "weights.pop(\"lm_head.decoder.weight\", None)\n",
        "\n",
        "reverse_model: GPT2LMHeadModel = GPT2LMHeadModel(cfg)\n",
        "reverse_model.load_state_dict(weights,strict=False)\n",
        "if device_r == 'cuda':\n",
        "    reverse_model.half()\n",
        "reverse_model.to(device_r)\n",
        "reverse_model.eval()\n",
        "\n",
        "\n",
        "end_token = torch.tensor([[50256]], dtype=torch.long)\n",
        "\n",
        "\n",
        "def _get_response(output_token, past):\n",
        "    out = torch.tensor([[]], dtype=torch.long, device=device_f)\n",
        "\n",
        "    while True:\n",
        "        util = model.forward(output_token, past_key_values=past)\n",
        "        output_token, past = util['logits'],util['past_key_values']\n",
        "        output_token = output_token[:, -1, :].float()\n",
        "        indices_to_remove = output_token < torch.topk(output_token, top_k)[0][..., -1, None]\n",
        "        output_token[indices_to_remove] = -float('Inf')\n",
        "        output_token = torch.multinomial(F.softmax(output_token, dim=-1), num_samples=1)\n",
        "\n",
        "        out = torch.cat((out, output_token), dim=1)\n",
        "\n",
        "        if output_token.item() == end_token.item():\n",
        "            break\n",
        "\n",
        "    return out, past\n",
        "\n",
        "\n",
        "def _score_response(output_token, correct_token):\n",
        "    inputs = torch.cat((output_token, correct_token), dim=1)\n",
        "    mask = torch.full_like(output_token, -100, dtype=torch.long)\n",
        "    labels = torch.cat((mask, correct_token), dim=1)\n",
        "\n",
        "    score = -reverse_model(inputs, labels=labels)['loss'].float()\n",
        "\n",
        "    return score\n",
        "\n",
        "\n",
        "def append_messages(old_list: list, new_list: list, truncate_length=64):\n",
        "    for message in new_list:\n",
        "        if message != '':\n",
        "            input_token = tokenizer.encode(message, return_tensors='pt')\n",
        "            input_token = torch.cat((input_token, end_token), dim=1)\n",
        "            old_list.append(input_token)\n",
        "\n",
        "    if len(old_list) == 0:\n",
        "        old_list.append(end_token)\n",
        "\n",
        "    # truncate\n",
        "    total_length = 0\n",
        "    for i, message in enumerate(reversed(old_list)):\n",
        "        total_length += message.shape[1]\n",
        "        if total_length > truncate_length:\n",
        "            old_list[:] = old_list[-i:]\n",
        "\n",
        "\n",
        "def generate_message(message_list: list, focus_last_message=True):\n",
        "    total_input = torch.cat(message_list, dim=1).to(device_f)\n",
        "    if focus_last_message:\n",
        "        total_input_reversed = message_list[-1]\n",
        "    else:\n",
        "        total_input_reversed = torch.cat(list(reversed(message_list)), dim=1)\n",
        "\n",
        "    past = None\n",
        "    if total_input.shape[1] > 1:\n",
        "        past = model(total_input[:, :-1])\n",
        "\n",
        "    results = []\n",
        "    for i in range(num_samples):\n",
        "        result = _get_response(total_input[:, -1:], past['past_key_values'])\n",
        "        score = _score_response(result[0].to(device_r), total_input_reversed.to(device_r))\n",
        "        results.append(result + (score,))\n",
        "\n",
        "    scores = torch.stack([x[2] for x in results], dim=0)\n",
        "    winner = torch.multinomial(F.softmax(scores / MMI_temperature, dim=0), num_samples=1).item()\n",
        "    # winner = torch.argmax(scores, dim=0)\n",
        "\n",
        "    out = results[winner][0]\n",
        "\n",
        "    return tokenizer.decode(out.tolist()[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "my_message_list = []\n",
        "while True:\n",
        "    print(\"usr >> \",end=\"\")\n",
        "    my_message = input()\n",
        "    if my_message==\"quit\":\n",
        "      print(\"bot >> Quit. Chating End\")\n",
        "      break\n",
        "    append_messages(my_message_list, [my_message])\n",
        "    my_response = generate_message(my_message_list)\n",
        "    print('bot >>', my_response)\n",
        "\n",
        "    append_messages(my_message_list, [my_response])"
      ],
      "metadata": {
        "id": "52c3rzyXEyBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lJuFzC_RKpyc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}